/* \n * To change this template, choose Tools | Templates\n * and open the template in the editor.\n */
$(document).ready(function(){
    init()
})

function init(){
    var text = "Scopus\nEXPORT DATE:01 Apr 2014\n\nGhoreishi, S.N., Sun, A.\nPredicting event-relatedness of popular queries\n(2013) International Conference on Information and Knowledge Management, Proceedings, pp. 1193-1196. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84889578658&partnerID=40&md5=b78d2d5ce3c603e30d72d4c3b8b53951\nABSTRACT: Many but not all popular queries are related to ongoing or recent events. In this paper, we identify 20 features including both contextual and temporal features from a small set of search results of a query and predict its event-relatedness. Search results from news and blog search engines are evaluated. Our analysis shows that the number of named entities in search results and their appearances in Wikipedia are among the most discriminative features for query event-relatedness prediction. Our study also shows that contextual features are more effective than temporal features. Evaluated with four classifiers (i.e., Support Vector Machine, Naïve Bayes, Multinomial Logistic Regression, and Bayesian Logistic Regression) on two datasets, our experiments show that query event-relatedness can be predicted with high accuracy using the proposed features. Copyright 2013 ACM.\nAUTHOR KEYWORDS: Event detection;  Query classification;  Query event relatedness\nINDEX KEYWORDS: Contextual feature; Discriminative features; Event detection; Logistic regressions; Multinomial logistic regression; Query classification; Query event relatedness; Temporal features, Image retrieval; Knowledge management; Regression analysis; Search engines; Websites, Forecasting\n\nDOI: 10.1145/2505515.2507853\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nLau, C.P., Zhang, X., Shihada, B.\nVideo quality prediction over wireless 4G\n(2013) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 7819 LNAI (PART 2), pp. 414-425. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84893525239&partnerID=40&md5=142adb40c7aa621a040241ce16b363c6\nABSTRACT: In this paper, we study the problem of video quality prediction over the wireless 4G network. Video transmission data is collected from a real 4G SCM testbed for investigating factors that affect video quality. After feature transformation and selection on video and network parameters, video quality is predicted by solving as regression problem. Experimental results show that the dominated factor on video quality is the channel attenuation and video quality can be well estimated by our models with small errors. © Springer-Verlag 2013.\nAUTHOR KEYWORDS: Superposition coded multicasting;  Video quality prediction;  Wireless 4G\nINDEX KEYWORDS: Channel attenuation; Feature transformations; Network parameters; Regression problem; Video quality; Video transmissions; Wireless 4G network, Data mining; Image communication systems; Multicasting, Forecasting\n\nDOI: 10.1007/978-3-642-37456-2_35\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nVinzamuri, B., Reddy, C.K.\nCox regression with correlation based regularization for electronic health records\n(2013) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 6729560, pp. 757-766. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84894654515&partnerID=40&md5=e6e78d19167fac694f5e427b39e60775\nABSTRACT: Survival Regression models play a vital role in analyzing time-to-event data in many practical applications ranging from engineering to economics to healthcare. These models are ideal for prediction in complex data problems where the response is a time-to-event variable. An event is defined as the occurrence of a specific event of interest such as a chronic health condition. Cox regression is one of the most popular survival regression model used in such applications. However, these models have the tendency to over fit the data which is not desirable for healthcare applications because it limits their generalization to other hospital scenarios. In this paper, we address these challenges for the cox regression model. We combine two unique correlation based regularizers with cox regression to handle correlated and grouped features which are commonly seen in many practical problems. The proposed optimization problems are solved efficiently using cyclic coordinate descent and Alternate Direction Method of Multipliers algorithms. We conduct experimental analysis on the performance of these algorithms over several synthetic datasets and electronic health records (EHR) data about heart failure diagnosed patients from a hospital. We demonstrate through our experiments that these regularizers effectively enhance the ability of cox regression to handle correlated features. In addition, we extensively compare our results with other regularized linear and logistic regression algorithms. We validate the goodness of the features selected by these regularized cox regression models using the biomedical literature and different feature selection algorithms. © 2013 IEEE.\nAUTHOR KEYWORDS: cox regression;  feature selection;  healthcare;  regularization\n\nDOI: 10.1109/ICDM.2013.89\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nTostes, A.I.J., Duarte-Figueiredo, F.D.L.P., Assunção, R., Salles, J., Loureiro, A.A.F.\nFrom data to knowledge: City-wide traffic flows analysis and prediction using bing maps\n(2013) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84884133216&partnerID=40&md5=8c0ae984803f5adc921b2c0b0913d7a4\nABSTRACT: Traffic jam is a common contemporary society issue in urban areas. City-wide traffic modeling, visualization, analysis, and prediction are still challenges in this context. Based on Bing Maps information, this work aims to acquire, aggregate, analyze, visualize, and predict traffic jam. Chicago area was evaluated as case study. The flow intensity (free or congested) was analyzed to allow the identification of phase transitions (shocks in the system). Also, a prediction model was developed based on logistic regression to correct discovery future flow intensities for a target street. © 2013 ACM.\nAUTHOR KEYWORDS: analysis;  human mobility;  traffic prediction;  urban computing;  visualization\nINDEX KEYWORDS: analysis; Flow intensity; Human mobility; Logistic regressions; Prediction model; Traffic model; Traffic prediction; Urban computing, Flow visualization; Traffic congestion; Visualization, Forecasting\n\nDOI: 10.1145/2505821.2505831\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nMahajan, D.K., Rastogi, R., Tiwari, C., Mitra, A.\nLogUCB: An explore-exploit algorithm for comments recommendation\n(2012) ACM International Conference Proceeding Series, pp. 6-15. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84871033924&partnerID=40&md5=45e8c2905f8fc2d82407d63d552ef8ac\nABSTRACT: The highly dynamic nature of online commenting environments makes accurate ratings prediction for new comments challenging. In such a setting, in addition to exploiting comments with high predicted ratings, it is also critical to explore comments with high uncertainty in the predictions. In this paper, we propose a novel upper confidence bound (UCB) algorithm called LOGUCB that balances exploration with exploitation when the average rating of a comment is modeled using logistic regression on its features. At the core of our LOGUCB algorithm lies a novel variance approximation technique for the Bayesian logistic regression model that is used to compute the UCB value for each comment. In experiments with a real-life comments dataset from Yahoo! News, we show that LOGUCB with bag-of-words and topic features outperforms state-of-the-art explore-exploit algorithms. © 2012 ACM.\nAUTHOR KEYWORDS: comment ratings;  explore-exploit;  logistic regression;  upper confidence bound\nINDEX KEYWORDS: Approximation techniques; Bag of words; Data sets; Dynamic nature; explore-exploit; Logistic regression models; Logistic regressions; Upper confidence bound, Knowledge management; Logistics; Regression analysis, Approximation algorithms\n\nDOI: 10.1145/2396761.2396767\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nWebber, W., Chandar, P., Carterette, B.\nAlternative assessor disagreement and retrieval depth\n(2012) ACM International Conference Proceeding Series, pp. 125-134. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84871086837&partnerID=40&md5=d4ad2be0d17b0ed1cb6f89731c0131ee\nABSTRACT: Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that a second assessor will disagree with the relevance assessment of the initial assessor, and find that there is a strong and consistent correlation between the two. We adopt a metarank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of second assessor disagreement given metarank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute system scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance. © 2012 ACM.\nAUTHOR KEYWORDS: evaluation;  retrieval experiment;  sampling\nINDEX KEYWORDS: Accurate prediction; evaluation; Logistic regression models; Logistic regressions; Model parameters; Predictive models; Relevance assessments; Retrieval systems; Statistical significance, Forecasting; Knowledge management; Logistics; Sampling, Regression analysis\n\nDOI: 10.1145/2396761.2396781\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nNurunnabi, A., West, G.\nOutlier detection in logistic regression: A quest for reliable knowledge from predictive modeling and classification\n(2012) Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012, art. no. 6406412, pp. 643-652. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84873142215&partnerID=40&md5=41e2cc3e8713a5597027077fd6a0d382\nABSTRACT: Logistic regression is well known to the data mining research community as a tool for modeling and classification. The presence of outliers is an unavoidable phenomenon in data analysis. Detection of outliers is important to increase the accuracy of the required estimates and for reliable knowledge discovery from the underlying databases. Most of the existing outlier detection methods in regression analysis are based on the single case deletion approach that is inefficient in the presence of multiple outliers because of the well known masking and swamping effects. To avoid these effects the multiple case deletion approach has been introduced. We propose a group deletion approach based diagnostic measure for identifying multiple influential observations in logistic regression. At the same time we introduce a plotting technique that can classify data into outliers, high leverage points, as well as influential and regular observations. This paper has two objectives. First, it investigates the problems of outlier detection in logistic regression, proposes a new method that can find multiple influential observations, and classifies the types of outlier. Secondly, it shows the necessity for proper identification of outliers and influential observations as a prelude for reliable knowledge discovery from modeling and classification via logistic regression. We demonstrate the efficiency of our method, compare the performance with the existing popular diagnostic methods, and explore the necessity of outlier detection for reliability and robustness in modeling and classification by using real datasets. © 2012 IEEE.\nAUTHOR KEYWORDS: Data mining;  High leverge point;  Influential observation;  Knowledge discovery;  Outlier;  Pattern recognition;  Regression;  Reliability;  Statistical computing\nINDEX KEYWORDS: High leverge point; Influential observations; Outlier; Regression; Statistical computing, Classification (of information); Data mining; Logistics; Pattern recognition; Regression analysis; Reliability, Statistics\n\nDOI: 10.1109/ICDMW.2012.107\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nKaladhar, D.S.V.G.K., Uma Devi, T., Lakshmi, P.V., Harikrishna Reddy, R., Sriteja Ayayangar V., R.K., Nageswara Rao, P.V.\nAnalysis of E.coli promoter regions using classification, association and clustering algorithms\n(2012) Advances in Intelligent and Soft Computing, 132 AISC, pp. 169-177. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84874452379&partnerID=40&md5=fc184d9f85b4e472a423d50edca29ea3\nABSTRACT: Data mining techniques can be well applied using various algorithms for the prediction of E.coli promoter regions. We studied various classification, Association and clustering algorithms like CART, Simple Logistic, BayesNet, Random forest, j48, LMT, Naïve Bayesian, Apriori and simpleKMeans over different E.coli promoter dataset. Random forest method using training dataset outperforms the remaining classification methods. The Association model (Apriori) predicted the presence of Adenine (A) at -45, -10 and -11 regions, Thiamine (T) at -35, -36 regions, Guanine (G) at -34 region. Cytosine (C) is not present in the submitted DNA data for E.coli promoter dataset at -14 to -9 and-36 to -31 regions using association model. Cluster based model using simpleKMeans predicted promoter regions true at -35 and -10 regions. If -36 to -31 region of the sequence contain TTGACA and -14 to -9 region contains TATAAT, there can be highest probability of finding promoter in E.coli. The condition becomes false, if the -36 to -31 region contains ACGACG and -14 to -9 contain TGAATG. © 2012 Springer-Verlag GmbH Berlin Heidelberg.\nAUTHOR KEYWORDS: Algorithms;  Data mining;  E.coli promoter\nINDEX KEYWORDS: Association models; Classification methods; Cluster-based; E.coli promoter; Promoter region; Random forest methods; Random forests; Training dataset, Algorithms; Classification (of information); Data mining; Decision trees; Information systems; Systems analysis, Clustering algorithms\n\nDOI: 10.1007/978-3-642-27443-5-20\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nVolkov, A., Van Den Poel, D.\nExtracting information from sequences of financial ratios with Markov for Discrimination: An application to bankruptcy prediction\n(2012) Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012, art. no. 6406460, pp. 340-343. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84873152098&partnerID=40&md5=337e81c79aeda8da39fa1d4be382e769\nABSTRACT: In this paper, we propose a method that extracts information from sequences of financial ratios and investigate the usefulness of this information for bankruptcy prediction, which constitutes an important class of financial services. We use the annual financial reports available from an external financial information services provider to extract predictors based on the Markov for Discrimination (MFD) methodology. These predictors are used as inputs in a binary classification model, which applies logistic regression to estimate the odds of bankruptcy. The results suggest that MFD-based predictors can achieve substantial predictive performance in terms of the AUC and the 5-percent predictive lift, which are two relevant performance metrics in our case. © 2012 IEEE.\nAUTHOR KEYWORDS: Bankruptcy predicition;  Financial services;  Markov for discrimination;  Sequence analysis\nINDEX KEYWORDS: Bankruptcy predicition; Bankruptcy prediction; Binary classification; Extracting information; Financial information; Financial ratios; Financial reports; Financial service; Logistic regressions; Markov for discrimination; Performance metrics; Predictive performance; Sequence analysis, Data mining; Information services; Logistics, Finance\n\nDOI: 10.1109/ICDMW.2012.137\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSibona, C., Brickey, J.\nA statistical comparison of classification algorithms on a single data set\n(2012) 18th Americas Conference on Information Systems 2012, AMCIS 2012, 6, pp. 4674-4685. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84878075959&partnerID=40&md5=af293cd565005d9a6ef8546b1e964a41\nABSTRACT: This research uses four classification algorithms in standard and boosted forms to predict members of a class for an online community. We compare two performance measures, area under the ROC (Receiver Operating Characteristic) curve (AUC) and accuracy in the standard and boosted forms. The research compares four popular algorithms Bayes, logistic regression, J48 and Nearest Neighbor (NN). The analysis shows that there are significant differences among the base classification algorithms-J48 had the best accuracy. Additionally, the results show that boosted methods improved the accuracy of logistic regression. ANOVA was used to detect the differences between the algorithms; post hoc analysis shows the differences between specific algorithms. © (2012) by the AIS/ICIS Administrative Office All rights reserved.\nAUTHOR KEYWORDS: Bayes;  Classification;  Ensemble methods;  Logistic regression;  Nearest neighbor\nINDEX KEYWORDS: Bayes; Classification algorithm; Ensemble methods; Logistic regressions; Nearest neighbors; Receiver operating characteristics; Significant differences; Statistical comparisons, Classification (of information); Information systems; Logistics; Regression analysis, Algorithms\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nLee, A., Levy, Y., Hafner, W.L.\nThe effect of information quality on trust in e-government systems\n(2012) 18th Americas Conference on Information Systems 2012, AMCIS 2012, 6, pp. 4869-4880. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84878028544&partnerID=40&md5=1297dfbcb31a6cfe8733d55fdad67c47\nABSTRACT: Over the past decades, the citizen-government relationship has changed. Citizens transitioned from traditional communications to interactions through e-government systems. While e-government systems expenditures increased, citizens' trust in e-government systems was a challenge. Moreover, although the role of information quality (IQ) as a contributor to trust in information systems (IS) received some attention, such role in e-government systems received limited attention. This two-phased study was designed to uncover citizens' perceived IQ factors, and determine their influence on trust in e-government systems. A list of e-government's IQ characteristics was developed and validated. Citizens were surveyed on their perceived importance level of IQ characteristics and trust in e-government systems. Exploratory factor analysis (EFA) based on 363 records obtained was used. Ordinal logistic regression (OLR) was used to formulate and test the predictive model. Results demonstrated two factors out of the three had a significant influence on trust. Results determined no significant gender differences. © (2012) by the AIS/ICIS Administrative Office All rights reserved.\nAUTHOR KEYWORDS: E-government systems;  Information quality;  Trust in e-government systems\nINDEX KEYWORDS: e-Government; E-government systems; Exploratory Factor Analysis (EFA); Gender differences; Information quality; Limited attentions; Ordinal logistic regression; Predictive models, Factor analysis; Information analysis; Information systems; Logistics, Government data processing\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nLiu, W., Chawla, S.\nA quadratic mean based supervised learning model for managing data skewness\n(2011) Proceedings of the 11th SIAM International Conference on Data Mining, SDM 2011, pp. 188-198. Cited 4 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84857182086&partnerID=40&md5=bd59691aa82bb969fc0f0d6e1811caed\nABSTRACT: In this paper, we study the problem of data skewness. A data set is skewed/imbalanced if its dependent variable is asymmetrically distributed. Dealing with skewed data sets has been identified as one of the ten most challenging problems in data mining research. We address the problem of class skewness for supervised learning models which are based on optimizing a regularized empirical risk function. These include both classification and regression models for discrete and continuous dependent variables. Classical empirical risk minimization is akin to minimizing the arithmetic mean of prediction errors, in which approach the induction process is biased towards the majority class for skewed data. To overcome this drawback, we propose a quadratic mean based learning framework (QMLearn) that is robust and insensitive to class skewness. We will note that minimizing the quadratic mean is a convex optimization problem and hence can be efficiently solved for large and high dimensional data. Comprehensive experiments demonstrate that the QMLearn model significantly outperforms existing statistical learners including logistic regression, support vector machines, linear regression, support vector regression and quantile regression etc. Copyright © SIAM.\nAUTHOR KEYWORDS: Convex optimization;  Data skewness;  Quadratic mean\nINDEX KEYWORDS: Convex optimization problems; Data skewness; Empirical risk minimization; High dimensional data; Logistic regressions; Quadratic mean; Quantile regression; Support vector regression (SVR), Convex optimization; Data mining; Logistics; Regression analysis; Supervised learning, Higher order statistics\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nMao, Y., Chen, Y., Hackmann, G., Chen, M., Lu, C., Kollef, M., Bailey, T.C.\nMedical data mining for early deterioration warning in general hospital wards\n(2011) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 6137495, pp. 1042-1049. Cited 2 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84863129844&partnerID=40&md5=b2807ba4cd0eb0f440a43149ca78ae2d\nABSTRACT: Data mining on medical data has great potential to improve the treatment quality of hospitals and increase the survival rate of patients. Every year, 4-17% of patients undergo cardiopulmonary or respiratory arrest while in hospitals. Early prediction techniques have become an apparent need in many clinical area. Clinical study has found early detection and intervention to be essential for preventing clinical deterioration in patients at general hospital units. In this paper, based on data mining technology, we propose an early warning system (EWS) designed to identify the signs of clinical deterioration and provide early warning for serious clinical events. Our EWS is designed to provide reliable early alarms for patients at the general hospital wards (GHWs). EWS automatically identifies patients at risk of clinical deterioration based on their existing electronic medical record. The main task of EWS is a challenging classification problem on high-dimensional stream data with irregular, multi-scale data gaps, measurement errors, outliers, and class imbalance. In this paper, we propose a novel data mining framework for analyzing such medical data streams. The framework addresses the above challenges and represents a practical approach for early prediction and prevention based on data that would realistically be available at GHWs. We assess the feasibility of the proposed EWS approach through retrospective study that includes data from 28,927 visits at a major hospital. Finally, we apply our system in a real-time clinical trial and obtain promising results. This project is an example of multidisciplinary cyber-physical systems involving researchers in clinical science, data mining, and nursing staff in the hospital. Our early warning algorithm shows promising result: the transfer of patients to ICU was predicted with sensitivity of 0.4127 and specificity of 0.950 in the real time system. © 2011 IEEE.\nAUTHOR KEYWORDS: Bootstrap aggregating;  Early warning system;  EMA (exponential moving average);  Exploratory undersampling;  Logistic regression\nINDEX KEYWORDS: Bootstrap aggregating; Early Warning System; Exponential moving averages; Logistic regression; Under-sampling, Deterioration; Embedded systems; Intensive care units; Interactive computer systems; Logistics; Medical computing; Medicine; Patient treatment; Real time systems, Data mining\n\nDOI: 10.1109/ICDMW.2011.117\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nKamishima, T., Akaho, S., Sakuma, J.\nFairness-aware learning through regularization approach\n(2011) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 6137441, pp. 643-650. Cited 3 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84857172480&partnerID=40&md5=888c50df2fd526982e8d5ffa64e166f4\nABSTRACT: With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect people's lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be socially and legally fair from a viewpoint of social responsibility; namely, it must be unbiased and nondiscriminatory in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. From a privacy-preserving viewpoint, this can be interpreted as hiding sensitive information when classification results are observed. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency. © 2011 IEEE.\nAUTHOR KEYWORDS: Classification;  Discrimination;  Fairness;  Information theory;  Logistic regression;  Privacy\nINDEX KEYWORDS: Analysis techniques; Classification results; Credit data; Credit scoring; Data mining technology; Discrimination; Discriminative models; Fairness; Logistic regression; Logistic regressions; Prediction algorithms; Privacy preserving; Regularization approach; Sensitive features; Sensitive informations; Social responsibilities; Statistical prediction, Algorithms; Classification (of information); Data privacy; Information theory; Logistics; Regression analysis, Data mining\n\nDOI: 10.1109/ICDMW.2011.83\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nCorrea, A.B., Gonzalez, A.M.\nEvolutionary algorithms for selecting the architecture of a MLP Neural Network: A credit scoring case\n(2011) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 6137452, pp. 725-732. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84857162175&partnerID=40&md5=bb6ca1b6dd4afa3f0147027c41ac2641\nABSTRACT: Neural Networks are powerful tools for classification and Regression, but it is difficult and time consuming to determine the best architecture for a given problem. In this paper two evolutionary algorithms, Genetic Algorithms (GA) and Binary Particle Swarm Optimization (BPS), are used to optimize the architecture of a Multi-Layer Perceptron Neural Network (MLP), in order to improve the predictive power of the credit risk scorecards. Results show that both methods outperform the Logistic Regression and a default neural network in terms of predictability, but the GA are more time consuming than the BPS. The predictive power of both methods is similar to the Global Optimum but it is found in a reasonable time. © 2011 IEEE.\nAUTHOR KEYWORDS: Credit scoring;  Genetic algorithm;  Neural Networks;  Particle Swarm Optimization\nINDEX KEYWORDS: Binary particle swarm optimization; Credit risks; Credit scoring; Global optimum; Logistic regressions; MLP neural networks; Multi-layer perceptron neural networks; Particle swarm; Predictive power, Data mining; Genetic algorithms; Logistics; Risk assessment, Neural networks\n\nDOI: 10.1109/ICDMW.2011.80\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nProceedings of International Conference on Information Systems for Crisis Response and Management, ISCRAM 2011\n(2011) Proceedings of International Conference on Information Systems for Crisis Response and Management, ISCRAM 2011, 599 p. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84860993065&partnerID=40&md5=1447da0243d28c2457380c7dc9579b53\nABSTRACT: The proceedings contain 105 papers. The topics discussed include: a research on responding capacity assessment of city power emergency engineering management based on the third indicator connotation; an empirical research on influence factors of China's state-owned enterprises M&A financing risk; constructing industry safety evaluation system under the influence of transnational corporations; empirical research on the relationship between energy consumption and economic growth in Heilongjiang province; evaluation and promotion strategy research on information communication ability of public crisis under the background of public relations in the crisis; marketing outsourcing risk assessment in the real estate based on risk matrix model; reliability prediction and Markov analysis of braking system based on Relex 2009; and research on demand transmission process risk management of supply chain based on fractal theory.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\nZurada, J.\nPredicting the risk of low back disorders due to manual handling tasks\n(2011) Proceedings of the Annual Hawaii International Conference on System Sciences, art. no. 6149018, pp. 1080-1088. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84857963249&partnerID=40&md5=23a16bf8f4b16ac61f844b31718a4728\nABSTRACT: Work related low back disorders (LBDs) due to manual material handling (MMH) tasks have long been recognized as one of the main occupational disabling injury that affects the quality of life of the industrial working population in the U.S. One of the efforts to comprehend the nature and phenomenon of LBDs due to MMH tasks was undertaken by Marras [18]. Based on multiple experiments they created a seminal data set and used it to build logistic regression models to identify significant variables and classify manual lifting tasks into high risk and low risk with respect to LBDs. Since then a number of studies have used the same data set to build and test various classification models. This paper analyzes the previous studies and employs the same data set to build and test seven classification methods. Though the performances of our best models are better than those reported in National Institute for Occupational Health and Safety (NIOHS) Guides and two of our previous studies, they are generally less optimistic than those reported in several other studies; this paper proposes a more systematic and reliable approach to creating and validating classifiers to distinguish between low and high risk manual lifting jobs that contribute to LBDs. © 2012 IEEE.\nINDEX KEYWORDS: Best model; Classification methods; Classification models; Data sets; Logistic regression models; Low back disorder; Manual handling; Manual lifting; Manual lifting tasks; Manual material handling; Occupational health and safety; Quality of life; Work-related; Working population, Classification (of information); Industrial hygiene; Logistics; Regression analysis; Statistical tests; Systems science, Ergonomics\n\nDOI: 10.1109/HICSS.2012.482\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSlutsky, A., An, Y., Hu, T., Burstyn, I.\nAutomatic approaches to clustering occupational description data for prediction of probability of workplace exposure to beryllium\n(2011) Proceedings - 2011 IEEE International Conference on Granular Computing, GrC 2011, art. no. 6122664, pp. 596-601. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84856795783&partnerID=40&md5=87d7c2207892896635f0ed21256e1c34\nABSTRACT: We investigated automatic approaches for clustering data that describes occupations related to hazardous airborne exposure (beryllium). The regulatory compliance data from Occupational Safety and Health Administration includes records containing short free text job descriptions and associated numerical exposure levels. Researchers in public health domain need to map job descriptions to Standard Occupational Classification (SOC) nomenclature for estimating occupational health risks. Previous manual process was time-consuming and did not advance so far to linkage to SOC. We investigated alternative automatic approaches for clustering job descriptions. The clustering results are the first essential step towards discovery of corresponding SOC terms. Our study indicated that the Tolerance Rough Set with Jaccard similarity was a better combination overall. The utility of the algorithm was further verified by applying logistic regression and validating that the predictive power of the automatically generated classifications, in terms of association of \"job\" with probability of exposure to beryllium above certain threshold, closely approached that of the manually assembled classification of the same 12,148 records. © 2011 IEEE.\nINDEX KEYWORDS: Airborne exposure; Automatically generated; Clustering data; Clustering results; Exposure level; Free texts; Job description; Logistic regressions; Manual process; Occupational health risk; Occupational safety and health administrations; Predictive power; Public health; Rough set; Workplace exposure, Clustering algorithms; Employment; Granular computing; Industrial hygiene; Job analysis; Logistics; Regulatory compliance, Beryllium\n\nDOI: 10.1109/GRC.2011.6122664\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nKim, J., Bae, J., Delen, D.\nUnderstading black boxes : Knowledge induction from models\n(2011) PACIS 2011 - 15th Pacific Asia Conference on Information Systems: Quality Research in Pacific, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84862922913&partnerID=40&md5=c1e889fbcfd4c01b4f4082412204626d\nABSTRACT: Due to regurations and laws prohibiting uses of private data on customers and their transactions in customer data base, most customer data sets are not easily accessable even in the same organizations. A solutio for this reguatory problems can be providing statistical summary of the data or models induced from the dat, instead of providing raw data sets. The models, however, have limited information on the original raw data set. This study explores possible solutions for these problems. The study uses prediction models from data on credit information of customers provided by a local bank in Seoul, S. Korea. This study suggests approaches in figuring what is inside of the non-rules based models such as regression models or neural network models. The study proposes several rule accumulation algorithms such as (RAA) and a GA-based rule refinement algorithm (GA-RRA) as possible solutions for the problems. The experiments show the performance of the random dataset, RAA, elimination of redundant rules (ERR), and GA-RRA.\nAUTHOR KEYWORDS: Ga-based rule refinement algorithm (GA-RRA);  Logistic regression model;  Personal credit rating;  Rule accumulation algorithm (RAA)\nINDEX KEYWORDS: Black boxes; Customer data; Data sets; Limited information; Local banks; Logistic regression model; Neural network model; Personal credit; Possible solutions; Prediction model; Private data; Redundant rules; Regression model; Rule accumulation algorithm (RAA); Rule refinement; Statistical summary, Customer satisfaction; Gallium; Information systems; Mathematical models; Neural networks; Regression analysis; Sales, Genetic algorithms\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSun, Y., Yan, X.\nAn estimation of the diffusion of internet: The case of Heilongjiang Province in China\n(2011) PACIS 2011 - 15th Pacific Asia Conference on Information Systems: Quality Research in Pacific, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84862954540&partnerID=40&md5=7ac35b7436d4f9240efb925cdc05ab9e\nABSTRACT: Based on the theory of innovation diffusion, we use Bass, Exponential, Logistic and Gompertz models to analyse the Internet diffusion of Heilongjiang Province and try to compare and find a suitable model that can describe its current Internet diffusion state. The result shows that Gompertz model has the best performance in describing its current diffusion. We found that intimation effect is low in the Internet diffusion. Also we found that the Internet diffusion rate will continue to increase and the maximum rate of diffusion is predicted to appear around the year of 2035, and the diffusion will end with about 52.1% penetrated rate. We provide some suggestions for the decision of provincial communications agency (PCA), which will also have reference value for other similar regions.\nAUTHOR KEYWORDS: Growth curve;  Innovation diffusion;  Internet\nINDEX KEYWORDS: Gompertz model; Growth curves; Innovation diffusion; Internet diffusion; Reference values, Information systems; Internet, Diffusion\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nIntelligent Data Engineering and Automated Learning, IDEAL 2011 - 12th International Conference, Proceedings\n(2011) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 6936 LNCS, 510 p. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052995737&partnerID=40&md5=f78412e981726475abe6e4b475b136d7\nABSTRACT: The proceedings contain 59 papers. The topics discussed include: on domain independence of author identification; interpreting hidden neurons in Boolean constructive neural networks; a language specification tool for model-based parsing; SVM approach to classifying lesions in USG images with the use of the Gabor decomposition; on the extraction and classification of hand outlines; clustering-based leaders' selection in multi-objective particle swarm optimisation; logistic fitting method for detecting onset and cessation of tree stem radius increase; novelty detection for identifying deterioration in emergency department patients; a modified apriori algorithm for analysing high-dimensional gene data; comparing multi-class classifiers: on the similarity of confusion matrices for predictive toxicology applications; automated image annotation system based on an open source object database; and a novel ensemble of distance measures for feature evaluation: application to sonar imagery.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\nPradel, B., Sean, S., Delporte, J., Guérif, S., Rouveirol, C., Usunier, N., Fogelman-Soulié, F., Dufau-Joel, F.\nA case study in a recommender system based on purchase data\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 377-385. Cited 2 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052651383&partnerID=40&md5=d3e3f74010fd79d5c2f306ca97b224ed\nABSTRACT: Collaborative filtering has been extensively studied in the context of ratings prediction. However, industrial recommender systems often aim at predicting a few items of immediate interest to the user, typically products that (s)he is likely to buy in the near future. In a collaborative filtering setting, the prediction may be based on the user's purchase history rather than rating information, which may be unreliable or unavailable. In this paper, we present an experimental evaluation of various collaborative filtering algorithms on a real-world dataset of purchase history from customers in a store of a French home improvement and building supplies chain. These experiments are part of the development of a prototype recommender system for salespeople in the store. We show how different settings for training and applying the models, as well as the introduction of domain knowledge may dramatically influence both the absolute and the relative performances of the different algorithms. To the best of our knowledge, the influence of these parameters on the quality of the predictions of recommender systems has rarely been reported in the literature. Copyright 2011 ACM.\nAUTHOR KEYWORDS: Algorithms;  Experimentation;  Performance\nINDEX KEYWORDS: Collaborative filtering; Collaborative filtering algorithms; Data sets; Domain knowledge; Experimental evaluation; Experimentation; Performance; Rating information; Relative performance, Algorithms; Data mining; Experiments; Forecasting; Sales, Recommender systems\n\nDOI: 10.1145/2020408.2020470\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nShao, X., Li, L.\nData-driven multi-touch attribution models\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 258-264. Cited 2 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052655970&partnerID=40&md5=68c13e9596f951e9a948745840018b60\nABSTRACT: In digital advertising, attribution is the problem of assigning credit to one or more advertisements for driving the user to the desirable actions such as making a purchase. Rather than giving all the credit to the last ad a user sees, multi-touch attribution allows more than one ads to get the credit based on their corresponding contributions. Multi-touch attribution is one of the most important problems in digital advertising, especially when multiple media channels, such as search, display, social, mobile and video are involved. Due to the lack of statistical framework and a viable modeling approach, true data-driven methodology does not exist today in the industry. While predictive modeling has been thoroughly researched in recent years in the digital advertising domain, the attribution problem focuses more on accurate and stable interpretation of the influence of each user interaction to the final user decision rather than just user classification. Traditional classification models fail to achieve those goals. In this paper, we first propose a bivariate metric, one measures the variability of the estimate, and the other measures the accuracy of classifying the positive and negative users. We then develop a bagged logistic regression model, which we show achieves a comparable classification accuracy as a usual logistic regression, but a much more stable estimate of individual advertising channel contributions. We also propose an intuitive and simple probabilistic model to directly quantify the attribution of different advertising channels. We then apply both the bagged logistic model and the probabilistic model to a real-world data set from a multi-channel advertising campaign for a well-known consumer software and services brand. The two models produce consistent general conclusions and thus offer useful cross-validation. The results of our attribution models also shed several important insights that have been validated by the advertising team. We have implemented the probabilistic model in the production advertising platform of the first author's company, and plan to implement the bagged logistic regression in the next product release. We believe availability of such data-driven multi-touch attribution metric and models is a break-through in the digital advertising industry. Copyright 2011 ACM.\nAUTHOR KEYWORDS: Bagged logistic regression;  Digital advertising;  Multi-touch attribution model\nINDEX KEYWORDS: Advertising campaign; Advertising industry; Bivariate; Classification accuracy; Classification models; Cross validation; Data-driven; Digital advertising; Logistic models; Logistic regression models; Logistic regressions; Media channel; Modeling approach; Multi-channel; Multi-touch; Predictive modeling; Probabilistic models; Product release; Real world data; Statistical framework; User classification; User interaction, Data mining; Regression analysis; Virtual reality, Marketing\n\nDOI: 10.1145/2020408.2020453\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nIfrim, G., Wiuf, C.\nBounded coordinate-descent for biological sequence classification in high dimensional predictor space\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 708-716. Cited 2 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052661040&partnerID=40&md5=8ada84e43b93e302ed09f09f0c459a6b\nABSTRACT: We present a framework for discriminative sequence classification where linear classifiers work directly in the explicit high-dimensional predictor space of all subsequences in the training set (as opposed to kernel-induced spaces). This is made feasible by employing a gradient-bounded coordinatedescent algorithm for efficiently selecting discriminative subsequences without having to expand the whole space. Our framework can be applied to a wide range of loss functions, including binomial log-likelihood loss of logistic regression and squared hinge loss of support vector machines. When applied to protein remote homology detection and remote fold recognition, our framework achieves comparable performance to the state-of-the-art (e.g., kernel support vector machines). In contrast to state-of-the-art sequence classifiers, our models are simply lists of weighted discriminative subsequences and can thus be interpreted and related to the biological problem - a crucial requirement for the bioinformatics and medical communities. Copyright 2011 ACM.\nAUTHOR KEYWORDS: Greedy coordinate-descent;  Logistic regression;  Sequence classification;  String classification;  Support vectormachines\nINDEX KEYWORDS: Biological problems; Biological sequences; Fold recognition; Greedy coordinate-descent; High-dimensional; Linear classifiers; Log likelihood; Logistic regression; Logistic regressions; Loss functions; Medical community; Remote homology; Sequence classification; String classification; Training sets, Bioinformatics; Data mining; Regression analysis; Support vector machines, Classification (of information)\n\nDOI: 10.1145/2020408.2020519\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nMenon, A.K., Chitrapura, K.-P., Garg, S., Agarwal, D., Kota, N.\nResponse prediction using collaborative filtering with hierarchies and side-information\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 141-149. Cited 5 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052679175&partnerID=40&md5=7f7952b73d548d47bde5262a17de105c\nABSTRACT: In online advertising, response prediction is the problem of estimating the probability that an advertisement is clicked when displayed on a content publisher's webpage. In this paper, we show how response prediction can be viewed as a problem of matrix completion, and propose to solve it using matrix factorization techniques from collaborative filtering (CF). We point out the two crucial differences between standard CF problems and response prediction, namely the requirement of predicting probabilities rather than scores, and the issue of confidence in matrix entries. We address these issues using a matrix factorization analogue of logistic regression, and by applying a principled confidence-weighting scheme to its objective. We show how this factorization can be seamlessly combined with explicit features or side-information for pages and ads, which let us combine the benefits of both approaches. Finally, we combat the extreme sparsity of response prediction data by incorporating hierarchical information about the pages and ads into our factorization model. Experiments on three very large real-world datasets show that our model outperforms current state-of-the-art methods for response prediction. Copyright 2011 ACM.\nAUTHOR KEYWORDS: Collaborative filtering;  Hierarchies;  Response prediction\nINDEX KEYWORDS: Collaborative filtering; Factorization model; Hierarchical information; Hierarchies; Logistic regressions; matrix; Matrix completion; Matrix factorizations; Online advertising; Real-world datasets; Response prediction; Side-information; State-of-the-art methods; Web-page, Data mining; Factorization; Matrix algebra, Forecasting\n\nDOI: 10.1145/2020408.2020436\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSimon, G.J., Kumar, V., Li, P.W.\nA simple statistical model and association rule filtering for classification\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 823-831. Cited 3 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052686087&partnerID=40&md5=272459db1899255350b06a2dda20f46a\nABSTRACT: Associative classification is a predictive modeling technique that constructs a classifier based on class association rules (also known as predictive association rules; PARs). PARs are association rules where the consequence of the rule is a class label. Associative classification has gained substantial research attention because it successfully joins the benefits of association rule mining with classification. These benefits include the inherent ability of association rule mining to extract high-order interactions among the predictors-an ability that many modern classifiers lack-and also the natural interpretability of the individual PARs. Associative classification is not without its caveats. Association rule mining often discovers a combinatorially large number of association rules, eroding the interpretability of the rule set. Extensive effort has been directed towards developing interestingness measures, which filter (predictive) association rules after they have been generated. These interestingness measures, albeit very successful at selecting interesting rules, lack two features that are highly valuable in the context of classification. First, only few of the interestingness measures are rooted in a statistical model. Given the distinction between a training and a test data set in the classification setting, the ability to make statistical inferences about the performance of the predictive classification rules on the test set is highly desirable. Second, the unfiltered set of predictive assocation rules (PARs) are often redundant, we can prove that certain PARs will not be used to construct a classification model given the presence of other PARs. In this paper, we propose a simple statistical model towards making inferences on the test set about the various performance metrics of predictive association rules. We also derive three filtering criteria based on hypothesis testing, which are very selective (reduce the number of PARs to be considered by the classifier by several orders of magnitude), yet do not effect the performance of the classification adversely. In the case, where the classification model is constructed as a logistic model on top of the PARs, we can mathematically prove, that the filtering criteria do not significantly effect the classifier's performance. We also demonstrate empirically on three publicly available data sets that the vast reduction in the number of PARs indeed did not come at the cost of reducing the predictive performance. Copyright 2011 ACM.\nINDEX KEYWORDS: Association rule mining; Associative classification; Class labels; Classification models; Classification rules; Data sets; Filtering criteria; High-order; Hypothesis testing; Interesting rules; Interestingness measures; Interpretability; Logistic models; Orders of magnitude; Performance metrics; Predictive association rule; Predictive modeling; Predictive performance; Rule set; Statistical inference; Statistical models; Test data; Test sets, Association reactions; Association rules; Data mining; Filtration; Statistical tests, Classification (of information)\n\nDOI: 10.1145/2020408.2020550\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nProceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD'11\n(2011) Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1458 p. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-80052651887&partnerID=40&md5=1accffa2f25dc6bd14d522d60b9855df\nABSTRACT: The proceedings contain 163 papers. The topics discussed include: CHIRP: a new classifier based on composite hypercubes on iterated random projections; supervised learning for provenance-similarity of binaries; trading representability for scalability: adaptive multi-hyperplane machine for nonlinear classification; an improved GLMNET for L1-regularized logistic regression; integrating low-rank and group-sparse structures for robust multi-task learning; model order selection for Boolean matrix factorization; rank aggregation via nuclear norm minimization; large-scale matrix factorization with distributed stochastic gradient descent; diversity in ranking via resistive graph centers; scalable distributed inference of dynamic user interests for behavioral targeting; multiple domain user personalization; click shaping to optimize multiple objectives; response prediction using collaborative filtering with hierarchies and side-information; and selecting a comprehensive set of reviews.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\nQing, X., Wang, X.\nA sequential dynamic multi-class model and recursive filtering by variational Bayesian methods\n(2011) Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 6634 LNAI (PART 1), pp. 301-312. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-79957964878&partnerID=40&md5=ab01bdd0e59a5e51faadb7afff50f96a\nABSTRACT: Adaptive classification evolving over time is an important learning task that arises in many applications. In this paper, a sequential dynamic multi-class model (SDMM) is proposed for representing the multi-class adaptive learning task, which is based on the polychotomous response model and dynamic logistic regression. Multiple state chains in the SDMM are coupled by the observable labels and feature vectors. Each state chain is modeled as a first-order Markov process with time-varying covariance parameters for characterizing the non-stationary generating process of sequential labels. Augmented auxiliary variables are introduced for developing efficient inference procedures according to the popular data augmentation strategy. Variational Bayesian methods are applied to estimate the dynamic state variables and augmented auxiliary variables recursively. According to the results of recursive filtering procedures using mean-field approximation forms, one-step-ahead predicted probabilities are calculated by marginalizing the state variables. Experiment results based on both synthetic and real data show that the proposed model significantly outperforms the non-sequential static methods for the multi-class adaptive learning problems with missing labels. Encouraging results have been obtained by comprising well-known multi-class data stream algorithms. © 2011 Springer-Verlag.\nINDEX KEYWORDS: Adaptive classification; Adaptive learning; Auxiliary variables; Covariance parameters; Data augmentation; Data stream algorithms; Dynamic state; Feature vectors; First-order; Learning tasks; Logistic regressions; Mean field approximation; Multi-class; Nonstationary; Recursive filtering; Response model; State variables; Static method; Synthetic and real data; Time varying; Variational bayesian, Bayesian networks; Markov processes, Data mining\n\nDOI: 10.1007/978-3-642-20841-6-25\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nWong, W.-K., Oberst, I., Das, S., Moore, T., Stumpf, S., McIntosh, K., Burnett, M.\nEnd-user feature labeling: A locally-weighted regression approach\n(2011) International Conference on Intelligent User Interfaces, Proceedings IUI, pp. 115-124. Cited 5 times.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-79952755349&partnerID=40&md5=972ac45dd8d10717b23b1d8aea41db1d\nABSTRACT: When intelligent interfaces, such as intelligent desktop assistants, email classifiers, and recommender systems, customize themselves to a particular end user, such customizations can decrease productivity and increase frustration due to inaccurate predictions-especially in early stages, when training data is limited. The end user can improve the learning algorithm by tediously labeling a substantial amount of additional training data, but this takes time and is too ad hoc to target a particular area of inaccuracy. To solve this problem, we propose a new learning algorithm based on locally weighted regression for feature labeling by end users, enabling them to point out which features are important for a class, rather than provide new training instances. In our user study, the first allowing ordinary end users to freely choose features to label directly from text documents, our algorithm was both more effective than others at leveraging end users' feature labels to improve the learning algorithm, and more robust to real users' noisy feature labels. These results strongly suggest that allowing users to freely choose features to label is a promising method for allowing end users to improve learning algorithms effectively. © 2011 ACM.\nAUTHOR KEYWORDS: Feature labeling;  Intelligent interfaces;  Locally weighted logistic regression;  Machine learning\nINDEX KEYWORDS: End users; Feature labeling; Intelligent interface; Locally weighted logistic regression; Locally weighted regression; Machine learning; Text document; Training data; User study, Learning systems; Regression analysis; User interfaces, Learning algorithms\n\nDOI: 10.1145/1943403.1943423\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\n16th Americas Conference on Information Systems, AMCIS 2010, Volume 3\n(2010) 16th Americas Conference on Information Systems 2010, AMCIS 2010, 3, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84870522082&partnerID=40&md5=99cb682db0c5c6ed9e0667dcbd321586\nABSTRACT: The proceedings contain 81 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: conflict, conflict management and performance in virtual teams; challenges of teaching information quality; the influence of human factors on vulnerability to information security breaches; clinicians' emotions and telestroke use; information technology adoption in Latin American microenterprises; antecedents of sustainable management support for IT-related initiatives; collaboration and end-user information management tools; iterative development of professional knowledge intensive business processes; tutorial on latent growth models for longitudinal data analysis; net neutrality and its implications; information security practices in Latin America; risks and hidden costs; satisfaction with social networking sites; the dilemma of addressing SAP skills shortages in developing countries; decision support variables for reverse logistics; decision support variables for reverse logistics in econometric model; building community sustainability with geographic information systems; the impact of individual centrality and helping on knowledge sharing; the extended advertising network model; human interaction with structure in the computing environment; a cultural theory analysis of information systems adoption; integration for innovation; critical competencies for the Brazilian CIO; tacit knowledge transfer within organisations; a method for business sequential data prediction and Re-conceptualizing IS strategic alignment.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\nUddin, M.N., Quaddus, M., Islam, N.\nKnowledge asset and inter-organizational relationship in the performance of Australian beef supply chain\n(2010) PACIS 2010 - 14th Pacific Asia Conference on Information Systems, pp. 725-737. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84856007225&partnerID=40&md5=2d016e04c5379466d83e8ee620bd6e59\nABSTRACT: Supply Chain Management has become a strategic issue in firm's success where Knowledge Asset and inter-organizational system can play a substantial role. Given that Australian beef industry is production pushed and lags behind in productivity improvement, this research was carried out to study if knowledge Asset Management (KAM) and inter-organizational relationship structure in supply chain (SC) have any impact on the performance of Australian beef industry leading into improving the competitiveness of the industry. We utilize concepts from organizational theories and marketing literature in agribusiness to develop the formative/reflective constructs, their measurement scales, and then use partial least squares (PLS) based structural equation modeling (SEM) to test our hypotheses. Data were collected through a telephone survey of a total of 315 firms including input suppliers, producers, processors, and retailers in the beef industry of Western Australia and Queensland. The PLS analysis reveals that 'KAM, is the strongest predictor of SC performance, followed by 'transaction climate' and vertical coordination among the chain members. Result also shows that SC performance strongly influences the 'competitiveness' of the industry as a whole. Thus this study identifies significant strategic supply chain factors, which will enable the stakeholders to do appropriate planning and benchmarking to improve performance of Australian beef industry.\nAUTHOR KEYWORDS: Beef industry;  Inter-organizational relationship;  Knowledge asset management;  Supply chain\nINDEX KEYWORDS: Inter-organizational relationships; Inter-organizational systems; Knowledge assets; Measurement scale; Organizational theory; Partial least squares; PLS analysis; Productivity improvements; Queensland; Strategic issues; Structural equation modeling; Telephone survey; Western Australia, Asset management; Benchmarking; Competition; Industry; Information systems; Knowledge management; Meats; Productivity; Supply chains, Supply chain management\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\n16th Americas Conference on Information Systems, AMCIS 2010, Volume 7\n(2010) 16th Americas Conference on Information Systems 2010, AMCIS 2010, 7, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84870279177&partnerID=40&md5=042f8ecc02f6416334f6c6c50ce9b1a8\nABSTRACT: The proceedings contain 87 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: towards a human processual approach of business-IT alignment; the role of an effective IT intervention for micro-enterprises; prediction of RFID performance in supply chains; incentive and control mechanisms for mitigating relational risk in IT outsourcing relationships; information seeking in organisations; service identification through value chain analysis and prioritization; a method to assess value of integrated operations; an approach for trust and IS engagement in professional-orientated work; developing an information infrastructure for international trade; a theoretical approach to netsourcing research; a systems view of software requirement volatility; fad-like technology adoption as a social action; knowledge transfer challenges for universities and SMEs in the USA; the credibility crisis in IS; the gateways of information on the cloud era; peer-to-peer service quality and its consequences in virtual communities; common data exchange standards; teaching with free software, open formats, and collaborative culture; a process-oriented tool development in the open innovation paradigm; influence of brokering in data warehouse projects; teaching business intelligent with an executive dashboard; toward a user commitment continuum; considering visual artifacts in IT cultures; measuring flow experience of computer game players; quality of XBRL US GAAP taxonomy; projects as social movements and understanding awareness diffusion in microblogging.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\n16th Americas Conference on Information Systems, AMCIS 2010, Volume 2\n(2010) 16th Americas Conference on Information Systems 2010, AMCIS 2010, 2, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84870434226&partnerID=40&md5=1fee0f1016db3f5a97f2c607a89dd301\nABSTRACT: The proceedings contain 84 papers. The special focus in this conference is on business intelligence, business process modelling, project management, IT organizational learning and healthcare information technology. The topics include: emergent leadership, gender, and culture; the empirical investigation of a wiki based group system in organizations; identifying areas for risk sharing in IT outsourcing; improving the trust of users on social networking sites via self-construal traits; data analysis for healthcare; general knowledge supported news analysis for portfolio risk prediction; secure web development teaching modules; an integrated model of individual web security behavior; toward optimal churn management; CIO turnover, IS alignment and revolutionary change; knowledge creation and firm performance; conceptualizing Green IT Organizational Learning (GITOL); virtual worlds as adjustable environments for immersion in business meetings; the evolving influence of diversity and media in virtual organizations; adoption of sustainability in IT Services; social technologies; knowledge sharing initiatives in a Chinese professional services firm; developing an assessment process for a master of information systems program; outsourcing contact centres to a developing country; convolution of complex process models and ratios; antecedents to supply chain integration; specifying the software project risk construct; collective sensemaking in virtual teams; benchmarking web-based image retrieval; analyzing enterprise systems delivery modes for small and medium enterprises; the fair factories clearinghouse and publishing IS research in and about Latin America.\nDOCUMENT TYPE: Conference Review\nSOURCE: Scopus\n\nPark, S., Choi, S.\nBayesian aggregation of binary classifiers\n(2010) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 5693993, pp. 393-402. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-79951754713&partnerID=40&md5=b4ba9426e61fbed00b419d1bfe52ca9f\nABSTRACT: Multiclass classification problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods have been developed to aggregate binary classifiers, including voting heuristics, loss-based decoding, and probabilistic decoding methods, but a little work on the optimal aggregation has been done. In this paper we present a Bayesian method for optimally aggregating binary classifiers where class membership probabilities are determined by predictive probabilities. We model the class membership probability as a softmax function whose input argument is a linear combination of discrepancies between codewords and probability estimates obtained by the binary classifiers. We consider a lower bound on the softmax function, which is represented as a product of logistic sigmoids, and we formulate the problem of learning aggregation weights as a variational logistic regression. Predictive probabilities computed by variational logistic regression yield the class membership probabilities. We stress two notable advantages over existing methods in the viewpoint of complexity and overfitting. Numerical experiments on several datasets confirm its useful behavior. © 2010 IEEE.\nAUTHOR KEYWORDS: Classifier aggregation;  Multiclass classification;  Variational logistic regression\nINDEX KEYWORDS: Aggregation weights; Bayesian; Bayesian methods; Binary classifiers; Binary problems; Code-words; Data sets; Decoding methods; Existing method; Linear combinations; Logistic regressions; Lower bounds; Membership probability; Multiclass classification; Multiclass classification problems; Numerical experiments; Overfitting; Probability estimate; Sigmoids, Bayesian networks; Classifiers; Data mining; Decoding; Regression analysis, Probability\n\nDOI: 10.1109/ICDM.2010.81\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSemenovich, D., Morioka, N., Sowmya, A.\nEfficient additive models via the generalized lasso\n(2010) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 5693434, pp. 1228-1233. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-79951738612&partnerID=40&md5=ab0088fdaf7349d4dbb29eb30615589a\nABSTRACT: We propose a framework for learning generalized additive models at very little additional cost (a small constant) compared to some of the most efficient schemes for learning linear classifiers such as linear SVMs and regularized logistic regression. We achieve this through a simple feature encoding scheme followed by a novel approach to regularization which we term \"generalized lasso\". Addtive models offer an attractive alternative to linear models for many large scale tasks as they have significantly higher predictive power while remaining easily interpretable. Furthermore, our regularizations approach extends to arbitrary graphs, allowing, for example, to explicitly incorporate spatial information or similar priors. Traditional approaches for learning additive models, such as back fitting, do not scale to large datasets. Our new formulation of the resulting optimization problem allows us to investigate the use of recent accelerated gradient algorithms and demonstrate speed comparable to state of the art linear SVM training methods, making additive models suitable for very large problems. In our experiments we find that additive models consistently outperform linear models on various datasets. © 2010 IEEE.\nAUTHOR KEYWORDS: Additive models;  Generalized lasso;  Regularization;  Stochastic gradient\nINDEX KEYWORDS: Additional costs; Additive models; Arbitrary graphs; Data sets; Encoding schemes; Generalized additive model; Generalized lasso; Gradient algorithm; Large datasets; Linear classifiers; Linear model; Linear SVM; Logistic regressions; Optimization problems; Predictive power; Regularization; Spatial informations; State of the art; Stochastic gradient; Training methods, Data mining; Stochastic systems, Stochastic models\n\nDOI: 10.1109/ICDMW.2010.184\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nScholz-Reiter, B., Heger, J., Hildebrandt, T.\nGaussian processes for dispatching rule selection in production scheduling: Comparison of learning techniques\n(2010) Proceedings - IEEE International Conference on Data Mining, ICDM, art. no. 5693356, pp. 631-638. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-79951729668&partnerID=40&md5=d75b98eda6cc14fad9ef0b5399dddbb8\nABSTRACT: Decentralized scheduling with dispatching rules is applied in many fields of logistics and production, especially in semiconductor manufacturing, which is characterized by high complexity and dynamics. Many dispatching rules have been found, which perform well on different scenarios, however no rule has been found, which outperforms other rules across various objectives. To tackle this drawback, approaches, which select dispatching rules depending on the current system conditions, have been proposed. Most of these use learning techniques to switch between rules regarding the current system status. Since the study of Rasmussen [1] has shown that Gaussian processes as a machine learning technique have outperformed other techniques like neural networks under certain conditions, we propose to use them for the selection of dispatching rules in dynamic scenarios. Our analysis has shown that Gaussian processes perform very well in this field of application. Additionally, we showed that the prediction quality Gaussian processes provide could be used successfully. © 2010 IEEE.\nAUTHOR KEYWORDS: Classifier;  Dispatching rules;  Gaussian processes;  K;  Machine learning;  Neural networks;  Regression;  Scheduling\nINDEX KEYWORDS: Dispatching rules; Gaussian processes; K; Machine learning; Regression, Classifiers; Data mining; Gaussian noise (electronic); Learning algorithms; Learning systems; Neural networks; Production engineering; Scheduling; Semiconductor device manufacture, Gaussian distribution\n\nDOI: 10.1109/ICDMW.2010.19\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nSchmidt, N.-H., Erek, K., Kolbe, L.M., Zarnekow, R.\nPredictors of green IT adoption: Implications from an empirical investigation\n(2010) 16th Americas Conference on Information Systems 2010, AMCIS 2010, 6, pp. 4433-4444. \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84870385063&partnerID=40&md5=d5435feade2bb4794fb369af64a98592\nABSTRACT: The increasing attention towards the environmental impact of IT (information technology) demands reorientation from IT departments. Research has outlined that the importance of Green IT is related to IT business alignment and environmental initiatives while uncertainty potentially derives from missing standards, measurements, or missing internal support for Green IT. We assume that importance and uncertainty are main determinants for Green IT adoption. These theoretical assumptions still lack empirical validation. As a contribution to the ongoing discussion of Green IT, we analyze data from 116 IT departments on the predictors of Green IT adoption using multinomial logistic regression. It is shown that the outlined assumptions prove to be statistical significant. These findings contribute to existing knowledge on Green IT and provide practical recommendations for CIOs, IT managers, environmental and sustainability managers as well as new opportunities for further research in this field.\nAUTHOR KEYWORDS: Adoption;  Green IT;  Importance;  IT departments;  Multinomial logistic regression;  Uncertainty\nINDEX KEYWORDS: Adoption; Green IT; Importance; IT departments; Multinomial logistic regression; Uncertainty, Environmental impact; Information systems; Logistics; Managers; Regression analysis; Sustainable development, Uncertainty analysis\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nZhang, R.\nRegional logistics demand analysis based on gray system theory: A case study of Hangzhou City\n(2010) 3rd International Conference on Knowledge Discovery and Data Mining, WKDD 2010, art. no. 5432700, pp. 130-133. Cited 1 time.\nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-77952173275&partnerID=40&md5=d7c0184cdb1e5f879dea2a440822155a\nABSTRACT: Regional logistics demand increased significantly with economic development in China since the 90's. But how to predict exactly logistics demand is still a difficult problem. There are many impact factors in the regional logistics demand, among these affect each other, mutual restraint. Changes in the course of logistics demand are a typical gray system. Gray Relational Analysis is to identify the different degree among the factors to deal with the original data and find the change rule. In this paper, Gray Relational Analysis analyzes relativity among some impact factors on the regional logistics demand in the different period. Comparative analysis found that factors at different period of time will be very different. The conclusion contributes to the logistics organization conducting further logistics demand analysis and forecasting. © 2010 IEEE.\nAUTHOR KEYWORDS: Gray relation analysis;  Gray system theory;  Logistics demand\nINDEX KEYWORDS: Comparative analysis; Economic development; Gray relation analysis; Gray relational analysis; Gray system; Gray system theory; Hangzhou City; Impact factor; Logistics demand; Regional logistics, Data mining; Indexing (of information); System theory, Logistics\n\nDOI: 10.1109/WKDD.2010.116\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus\n\nEllison, R.J., Woody, C.\nSupply-chain risk management: Incorporating security into software development\n(2010) Proceedings of the Annual Hawaii International Conference on System Sciences, art. no. 5428501, . \nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-77951740753&partnerID=40&md5=5fd87bf8cedc1e86c86274c90ee6ec2c\nABSTRACT: As outsourcing and expanded use of commercial off-the-shelf (COTS) products increase, supply-chain risk becomes a growing concern for software acquisitions. Supply-chain risks for hardware procurement include manufacturing and delivery disruptions,1 and the substitution of counterfeit or substandard components. Software supply-chain risks include third-party tampering with a product during development or delivery, and, more likely, a compromise of the software assurance through the introduction of software defects. This paper describes practices that address such defects and mechanisms for introducing these practices into the acquisition life cycle. The practices improve the likelihood of predictable behavior by systematically analyzing data flows to identify assumptions and using knowledge of attack patterns and vulnerabilities to analyze behavior under conditions that an attacker might create. © 2010 IEEE.\nINDEX KEYWORDS: Attack patterns; Commercial off-the-shelf products; Data flow; Software acquisition; Software assurance; Software defects; Software development, Computer software; Data flow analysis; Defects; Risk management; Risks; Software design, Supply chain management\n\nDOI: 10.1109/HICSS.2010.355\nDOCUMENT TYPE: Conference Paper\nSOURCE: Scopus";
    console.log(text.split(/[\r\n]\s*[\r\n]/g))
    $("#thefile").on("change", function(){
        var reader = new FileReader();
        reader.onload = function(e){
            text = e.target.result;
            var dummyarray = [];
            dummyarray.push(text);
            console.log(dummyarray);
            console.log(text.split(/[\r\n]\s*[\r\n]/g))
        }
    
        reader.readAsText(document.getElementById("thefile").files[0]);  
        
        
    })
    
}
